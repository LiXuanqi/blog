---
title: CS336 Notes
date: 2025-12-28
description: ""
tags: ["ML"]
visible: true
---

[Playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)

# Lecture 1: Overview, tokenization

## It's all about efficiency

accuracy = efficiency \* resources

We are compute-constrained in this course, But it's data-constrained in real world.

efficiency drives design decisions.

Algorithms that scale is what matters.

## Tokenization

A Tokenizer is a class that implements the encode and decode methods.

The vocabulary size is number of possible tokens (integers).

[iteractive site](https://tiktokenizer.vercel.app/?encoder=gpt2)

observations:

- A word and its preceding space are part of the same token (e.g., " world").
- A word at the beginning and in the middle are represented differently (e.g., "hello hello").
- Numbers are tokenized into every few digits. e.g. (12345678 => 123|456|78)

### Compression ratio

### Character-based tokenization

```python
assert ord("a") == 97
assert chr(97) == "a"
```

Cons:

- large vocabulary
- many characters are quite rare (e.g., üåç), which is inefficient use of the vocabulary

### Byte-based tokenization

Any string can be encoded as bytes.
e.g.

```python
assert bytes("a", encoding="utf-8") == b"a"
assert bytes("üåç", encoding="utf-8") == b"\xf0\x9f\x8c\x8d"
```

Pros:

- vocabulary is small. 1 byte can have 256 values.

Cons:

- compression_ratio == 1, which is terrible. The sequences will be too long.

### Word-based tokenization

Steps:

1. Split strings into words
2. map segments into integers

Cons:

- the number of words is huge
- Many words are rare
- This doesn't obviously provide a fixed vocabulary size.
- New words we haven't seen during training get a special UNK token, which is ugly and can mess up perplexity calculations.

### Byte-Pair Encoding (BPE) tokenizer

Basic idea: train the tokenizer on raw text to automatically determine the vocabulary.

Intuition: common sequences of characters are represented by a single token, rare sequences are represented by many tokens.

Sketch: start with each byte as a token, and successively merge the most common pair of adjacent tokens.

## Lecture 2: PyTorch, resource accounting
